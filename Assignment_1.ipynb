{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Installations, Imports and Helper Functions"
      ],
      "metadata": {
        "id": "FpMnIYAAiW0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install accelerate -U\n",
        "!pip install livelossplot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kExddI_htHc",
        "outputId": "0945e0cf-fcb0-455e-9166-58dadee3a419"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "Successfully installed accelerate-0.33.0\n",
            "Collecting livelossplot\n",
            "  Downloading livelossplot-0.5.5-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.7.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.4.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.26.4)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (24.1)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2.1.4)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2024.6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
            "Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "import optax\n",
        "import math\n",
        "from livelossplot import PlotLosses\n"
      ],
      "metadata": {
        "id": "wpg3jOXvgsoU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if os.environ[\"COLAB_GPU\"] and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "# https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html#gpu-memory-allocation\n",
        "# Avoid GPU memory allocation to be done by JAX.\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oRsev9i5JCL",
        "outputId": "b261bf71-12d0-4da4-e307-1cf4dd933c83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a GPU is connected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Data\n",
        "\n",
        "\n",
        "Load the data from text files"
      ],
      "metadata": {
        "id": "p5nM7LSjgdwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NguniDataset(Dataset):\n",
        "    def __init__(self, data, seq_length, special_tokens):\n",
        "        chars = sorted(list(set(data)))\n",
        "        self.char2idx = {char: idx for idx, char in enumerate(chars)}\n",
        "        self.char2idx.update({token: len(self.char2idx) for token in special_tokens})\n",
        "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
        "        self.data = [self.char2idx[char] for char in data]\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq = self.data[idx:idx + self.seq_length]\n",
        "        target_seq = self.data[idx + 1:idx + self.seq_length + 1]\n",
        "        return torch.tensor(input_seq), torch.tensor(target_seq)"
      ],
      "metadata": {
        "id": "2k9yQDN2f-mM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Nguni language data\n",
        "def load_nguni_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        text = f.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "GevKPs3hgstC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = '/content/drive/My Drive/data/'\n",
        "else:\n",
        "    base_path = './LMDatasets/'\n",
        "\n",
        "\n",
        "train_data_path = os.path.join(base_path, 'nchlt_text.nr.train')\n",
        "val_data_path = os.path.join(base_path, 'nchlt_text.nr.valid')\n",
        "test_data_path = os.path.join(base_path, 'nchlt_text.nr.test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY_PWCCYg0rW",
        "outputId": "6a3393de-ece4-4060-9c4e-cb89786badfc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multihead Attention"
      ],
      "metadata": {
        "id": "pa_USFm1qXQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceToQKV(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, X):\n",
        "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "\n",
        "    # this can also be one layer, how do you think you would do it?\n",
        "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ],
      "metadata": {
        "id": "IPLQA5EWrfUc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "    attention = jnp.matmul(attention_weights, value)\n",
        "    return attention, attention_weights"
      ],
      "metadata": {
        "id": "FKvPnnCTqfMl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "    self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "  def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "    if None in [Q, K, V]:\n",
        "      assert not X is None, \"X has to be provided if either Q,K,V not provided\"\n",
        "\n",
        "      # project all data to Q, K, V\n",
        "      Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "    # get the batch size, sequence length and embedding size\n",
        "    B, T, d_m = K.shape\n",
        "\n",
        "    # calculate heads embedding size (d_m/N)\n",
        "    head_size = d_m // self.num_heads\n",
        "\n",
        "    # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "    q_heads = Q.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    k_heads = K.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    v_heads = V.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "    attention, attention_weights = scaled_dot_product_attention(\n",
        "        q_heads, k_heads, v_heads, mask\n",
        "    )\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
        "    attention = attention.swapaxes(1, 2).reshape(B, -1, d_m)\n",
        "\n",
        "    # apply Wo\n",
        "    X_new = self.Wo(attention)\n",
        "\n",
        "    if return_weights:\n",
        "      return X_new, attention_weights\n",
        "    else:\n",
        "      return X_new"
      ],
      "metadata": {
        "id": "5fg38ruLqb5d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model:"
      ],
      "metadata": {
        "id": "aMcOR1fYmPhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, processed_x):\n",
        "\n",
        "    added = x + processed_x\n",
        "    normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
        "    return normalised(added)"
      ],
      "metadata": {
        "id": "68Y-b3sBpu1I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  \"\"\"A 2-layer MLP which widens then narrows the input.\"\"\"\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "    layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
        "    layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "FSTXQ6Zgp2CG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer decoder block.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    d_m: Token embedding size\n",
        "    widening factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "\n",
        "    X = self.add_norm1(X, attention)\n",
        "\n",
        "    projection = self.MLP(X)\n",
        "    X = self.add_norm2(X, projection)\n",
        "\n",
        "    return (X, attention_weights_1) if return_att_weight else X"
      ],
      "metadata": {
        "id": "tV1HRAVmlPkW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "  assert token_embedding % 2 == 0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "  i = jnp.arange(0, token_embedding, 2)\n",
        "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "  frequencies = positions * frequency_steps\n",
        "\n",
        "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "  return P"
      ],
      "metadata": {
        "id": "TJEKSrDewRDf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLM(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer encoder consisting of several layers of decoder blocks.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    num_layers: The number of decoder blocks to be used.\n",
        "    d_m: Token embedding size\n",
        "    vocab_size: The size of the vocabulary\n",
        "    widening_factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "  num_heads: int\n",
        "  num_layers: int\n",
        "  d_m: int\n",
        "  vocab_size: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.blocks = [\n",
        "        DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
        "        for _ in range(self.num_layers)\n",
        "    ]\n",
        "    self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m) # convert tokens to embedding\n",
        "    self.pred_layer = nn.Dense(self.vocab_size)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weights=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    # convert a token id to a d_m dimensional vector\n",
        "    X = self.embedding(X)\n",
        "    sequence_len = X.shape[-2]\n",
        "    positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "    X = X + positions\n",
        "\n",
        "    if return_att_weights:\n",
        "        att_weights = []\n",
        "    block_n = 0\n",
        "    for block in self.blocks:\n",
        "        out = block(X, mask, return_att_weights)\n",
        "        if return_att_weights:\n",
        "            X = out[0]\n",
        "            att_weights.append(out[1])\n",
        "        else:\n",
        "            X = out\n",
        "\n",
        "    # apply a linear layer and softmax to calculate our logits over tokens\n",
        "    logits = nn.log_softmax(self.pred_layer(X))\n",
        "\n",
        "    return (\n",
        "        logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))\n",
        "    )"
      ],
      "metadata": {
        "id": "wlOPBgqNhDs9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_loss_fn(logits, targets):\n",
        "  \"\"\"Compute the loss on data wrt params.\"\"\"\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  print(logits.shape,target_labels.shape)\n",
        "  assert logits.shape == target_labels.shape\n",
        "  mask = jnp.greater(targets, 0)\n",
        "  loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "  loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "mFVywROcs3Zg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run me to test your code\n",
        "VOCAB_SIZE = 25670\n",
        "targets = jnp.array([[0, 2, 0]])\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
        "loss = sequence_loss_fn(X, targets)\n",
        "real_loss = jnp.array(10.966118)\n",
        "assert jnp.allclose(real_loss, loss), \"Not returning the correct value\"\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6GMzTmo3jDw",
        "outputId": "9d80c34a-f303-461d-eb2b-0f249356963d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 25670) (1, 3, 25670)\n",
            "It seems correct. Look at the answer below to compare methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5KNC-Fn-tgQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CharBasedAsciiDatasetForLLM:\n",
        "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokenize by splitting the text into characters\n",
        "        characters = list(corpus)\n",
        "        self.vocab_size = len(set(characters))  # Number of unique characters\n",
        "\n",
        "        # Create a mapping from characters to unique IDs\n",
        "        self.char_to_id = {char: i for i, char in enumerate(set(characters))}\n",
        "\n",
        "        # Store the inverse mapping from IDs to characters\n",
        "        self.id_to_char = {i: char for char, i in self.char_to_id.items()}\n",
        "\n",
        "        # Convert the characters in the corpus to their corresponding IDs\n",
        "        corpus = np.array([self.char_to_id[char] for char in characters]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Only {num_batches} batches; consider a shorter \"\n",
        "                \"sequence or a smaller batch.\"\n",
        "            )\n",
        "\n",
        "        self._ds = CharBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Yield next mini-batch.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Create the language modeling observation/target pairs.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_chars(self, ids):\n",
        "        \"\"\"Convert a sequence of character IDs to characters.\"\"\"\n",
        "        return [self.id_to_char[id] for id in ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ],
      "metadata": {
        "id": "XIGcdGL78GmZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample and look at the data\n",
        "batch_size = 2\n",
        "seq_length = 32\n",
        "\n",
        "train_dataset = CharBasedAsciiDatasetForLLM(train_data_path, batch_size, seq_length)\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(\"CHAR:\", ' '.join(train_dataset.ids_to_chars(obs)))\n",
        "    print(\"ASCII:\", obs)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(\"CHAR:\", ' '.join(train_dataset.ids_to_chars(target)))\n",
        "    print(\"ASCII:\", target)\n",
        "\n",
        "print(f\"\\n Total vocabulary size: {train_dataset.vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya_tRxNP8V7I",
        "outputId": "65f8efa7-2936-4670-f612-38cfbdca368a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Input -----------\n",
            "CHAR:   i f o r o m o   l e s i b a w o   e k h a s i n i   e l i l a\n",
            "ASCII: [21  7  3  5 66  5 52  5 21 13 73 14  7 60 69 68  5 21 73  0 32 69 14  7\n",
            " 38  7 21 73 13  7 13 69]\n",
            "---------- Target ----------\n",
            "CHAR: i f o r o m o   l e s i b a w o   e k h a s i n i   e l i l a n\n",
            "ASCII: [ 7  3  5 66  5 52  5 21 13 73 14  7 60 69 68  5 21 73  0 32 69 14  7 38\n",
            "  7 21 73 13  7 13 69 38]\n",
            "---------- Input -----------\n",
            "CHAR: g o m b a n a   u m u m e t h e   i l w a z i   e l i q a k a t\n",
            "ASCII: [81  5 52 60 69 38 69 21 80 52 80 52 73 74 32 73 21  7 13 68 69 59  7 21\n",
            " 73 13  7 41 69  0 69 74]\n",
            "---------- Target ----------\n",
            "CHAR: o m b a n a   u m u m e t h e   i l w a z i   e l i q a k a t h\n",
            "ASCII: [ 5 52 60 69 38 69 21 80 52 80 52 73 74 32 73 21  7 13 68 69 59  7 21 73\n",
            " 13  7 41 69  0 69 74 32]\n",
            "\n",
            " Total vocabulary size: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "class WordBasedAsciiDatasetForLLM:\n",
        "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokenize by splitting the text into words\n",
        "        words = corpus.split()\n",
        "        self.vocab_size = len(set(words))  # Number of unique words\n",
        "\n",
        "        # Create a mapping from words to unique IDs\n",
        "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
        "\n",
        "        # Store the inverse mapping from IDs to words\n",
        "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
        "\n",
        "        # Convert the words in the corpus to their corresponding IDs\n",
        "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Only {num_batches} batches; consider a shorter \"\n",
        "                \"sequence or a smaller batch.\"\n",
        "            )\n",
        "\n",
        "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Yield next mini-batch.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Create the language modeling observation/target pairs.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        \"\"\"Convert a sequence of word IDs to words.\"\"\"\n",
        "        return [self.id_to_word[id] for id in ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOu_vAPOuIM-",
        "outputId": "a8f05dd9-28c4-40c3-8f47-97b8c46514b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-20 13:09:24--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-08-20 13:09:24 (47.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample and look at the data\n",
        "batch_size = 2\n",
        "seq_length = 32\n",
        "#train_dataset = WordBasedAsciiDatasetForLLM(train_data_path, batch_size, seq_length)\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(train_data_path, batch_size, seq_length)\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(obs)))\n",
        "    print(\"ASCII:\", obs)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(target)))\n",
        "    print(\"ASCII:\", target)\n",
        "\n",
        "print(f\"\\n Total vocabulary size: {train_dataset.vocab_size}\")\n",
        "\n",
        "VOCAB_SIZE = train_dataset.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIBRwujttifu",
        "outputId": "a8055e70-5fbf-4204-9ae5-fb30c00f2255"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Input -----------\n",
            "TEXT: engingene ngalo. Mina, njengelunga le-GeMS, ngikatelelekile ukubhadela imali yenyanga ku-GeMS. Mina namkha engibondlako akekho namunye ongumdlalifa wesinye isiKimu sezokwelapha esitlolisiweko. Ngizakwazisa isikimu ngamanye namanye amatjhuguluko emaphilweni wengibondlako namkha ngobujamo babo, njengombana kufunwa\n",
            "ASCII: [100922 119673  50335    551  79225  51778 103534 114503  67206 111547\n",
            "  68748  96433  36086  68406  41968 110881  64250  25518  15542 123461\n",
            " 100460 110517  13601 117055  44600  62575  47680  96433 109578   3320\n",
            "  38737 110673]\n",
            "---------- Target ----------\n",
            "TEXT: ngalo. Mina, njengelunga le-GeMS, ngikatelelekile ukubhadela imali yenyanga ku-GeMS. Mina namkha engibondlako akekho namunye ongumdlalifa wesinye isiKimu sezokwelapha esitlolisiweko. Ngizakwazisa isikimu ngamanye namanye amatjhuguluko emaphilweni wengibondlako namkha ngobujamo babo, njengombana kufunwa ngaphakathi\n",
            "ASCII: [119673  50335    551  79225  51778 103534 114503  67206 111547  68748\n",
            "  96433  36086  68406  41968 110881  64250  25518  15542 123461 100460\n",
            " 110517  13601 117055  44600  62575  47680  96433 109578   3320  38737\n",
            " 110673  25940]\n",
            "---------- Input -----------\n",
            "TEXT: ukuphathwa kwetjhejo neenhloso zokubika, begodu okhunye nokhunye ukuphambuka kilokhu kubanga umlandu wekwephulwa komThetho weFihlo. Ngiyezwisisa kobana imininingwana yami emayelana nezamaphilo ayizokusetjenziselwa iinhloso ezimayela nerhwebo namkha ithengiswe ngehloso yokwenza imali. Ngiyezwisisa kobana i-GeMS\n",
            "ASCII: [ 30900  26381  93975  46641  51529  48792 119367  64166  27756  25984\n",
            "  38561  19988  19614  96449   6694  99839  99427  74663  44506 113540\n",
            "  75933  36583  81531  65235  96433  54330  10476  42579  30648   6694\n",
            "  99839  45020]\n",
            "---------- Target ----------\n",
            "TEXT: kwetjhejo neenhloso zokubika, begodu okhunye nokhunye ukuphambuka kilokhu kubanga umlandu wekwephulwa komThetho weFihlo. Ngiyezwisisa kobana imininingwana yami emayelana nezamaphilo ayizokusetjenziselwa iinhloso ezimayela nerhwebo namkha ithengiswe ngehloso yokwenza imali. Ngiyezwisisa kobana i-GeMS inikele\n",
            "ASCII: [ 26381  93975  46641  51529  48792 119367  64166  27756  25984  38561\n",
            "  19988  19614  96449   6694  99839  99427  74663  44506 113540  75933\n",
            "  36583  81531  65235  96433  54330  10476  42579  30648   6694  99839\n",
            "  45020  54592]\n",
            "\n",
            " Total vocabulary size: 124198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(3, 4))\n",
        "def train_step(params, optimizer_state, batch, apply_fn, update_fn):\n",
        "  def loss_fn(params):\n",
        "    T = batch['input'].shape[1]\n",
        "    logits = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
        "    loss = sequence_loss_fn(logits, batch['target'])\n",
        "    return loss\n",
        "\n",
        "  loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
        "  updates, optimizer_state = update_fn(gradients, optimizer_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, optimizer_state, loss"
      ],
      "metadata": {
        "id": "cCrbh1gmvXEM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all hyperparameters\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "num_layers = 1\n",
        "widening_factor = 2\n",
        "LR = 2e-3\n",
        "batch_size = 32\n",
        "seq_length = 64\n",
        "\n",
        "# set up the data\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(train_data_path, batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "# initialise model\n",
        "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
        "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "params = llm.init(key, batch['input'], mask)\n",
        "\n",
        "# set up the optimiser\n",
        "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)"
      ],
      "metadata": {
        "id": "o9LF7eP9wi19"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all hyperparameters\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "num_layers = 1\n",
        "widening_factor = 2\n",
        "LR = 2e-3\n",
        "batch_size = 32\n",
        "seq_length = 64\n",
        "\n",
        "# set up the data\n",
        "train_dataset = CharBasedAsciiDatasetForLLM(train_data_path, batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "# initialise model\n",
        "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
        "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "params = llm.init(key, batch['input'], mask)\n",
        "\n",
        "# set up the optimiser\n",
        "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)"
      ],
      "metadata": {
        "id": "iCu2wAcI899G"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 3500\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "VOCAB_SIZE = train_dataset.vocab_size\n",
        "\n",
        "# Training loop\n",
        "for step in range(MAX_STEPS):\n",
        "    batch = next(train_dataset)\n",
        "    params, optimizer_state, loss = train_step(\n",
        "        params, optimizer_state, batch, llm.apply, optimizer.update)\n",
        "    losses.append(loss)\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "rpN9v7bqx4rC",
        "outputId": "d575c6cb-1722-4126-9886-6f6b3c2153cf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss\n",
            "\tloss             \t (min:    1.195, max:    5.087, cur:    1.480)\n"
          ]
        }
      ]
    }
  ]
}